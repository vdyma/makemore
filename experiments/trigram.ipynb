{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:12.127244Z",
     "start_time": "2024-03-29T11:21:10.259609Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:12.157848Z",
     "start_time": "2024-03-29T11:21:12.131229Z"
    }
   },
   "id": "cf77393bb9e11465",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(word) for word in words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:12.174378Z",
     "start_time": "2024-03-29T11:21:12.159830Z"
    }
   },
   "id": "4062a705ead35d53",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "special_token = \".\"\n",
    "alphabet = [special_token] + sorted(list(set(\"\".join(words))))\n",
    "itos = dict((i, s) for i, s in enumerate(alphabet))  # index to string\n",
    "stoi = dict((s, i) for i, s in itos.items())  # string to index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:12.189898Z",
     "start_time": "2024-03-29T11:21:12.175383Z"
    }
   },
   "id": "21494afad2db6ced",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_grams = 3\n",
    "\n",
    "shape = tuple(len(alphabet) for _ in range(num_grams))\n",
    "N = torch.zeros(shape, dtype=torch.int32)\n",
    "\n",
    "for word in words:\n",
    "    processed_word = [special_token, special_token] + list(word) + [special_token]\n",
    "    zipped_word = zip(*[processed_word[i:] for i in range(num_grams)])\n",
    "    for chars in zipped_word:\n",
    "        N[tuple(map(lambda x: stoi[x], chars))] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:14.954394Z",
     "start_time": "2024-03-29T11:21:12.190894Z"
    }
   },
   "id": "db7dea6618b52d16",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=2, keepdim=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:14.970116Z",
     "start_time": "2024-03-29T11:21:14.956580Z"
    }
   },
   "id": "9f83e034b30114c2",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3.1192e-05, 1.3759e-01, 4.0767e-02, 4.8129e-02, 5.2745e-02, 4.7785e-02,\n        1.3038e-02, 2.0898e-02, 2.7293e-02, 1.8465e-02, 7.5577e-02, 9.2452e-02,\n        4.9064e-02, 7.9195e-02, 3.5777e-02, 1.2321e-02, 1.6095e-02, 2.9008e-03,\n        5.1154e-02, 6.4130e-02, 4.0830e-02, 2.4641e-03, 1.1759e-02, 9.6070e-03,\n        4.2109e-03, 1.6719e-02, 2.9008e-02])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:15.001689Z",
     "start_time": "2024-03-29T11:21:14.974121Z"
    }
   },
   "id": "e08edcdcceeb0df8",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brona\n",
      "kercur\n",
      "jer\n",
      "li\n",
      "lum\n",
      "advtqjyouna\n",
      "suha\n",
      "mer\n",
      "trendoustaytel\n",
      "zade\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(0)\n",
    "\n",
    "for i in range(10):\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    name_chars = []\n",
    "    while True:\n",
    "        ix3 = torch.multinomial(P[ix1, ix2], 1, replacement=True, generator=g).item()\n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        name_chars.append(itos[ix3])\n",
    "        ix1 = ix2\n",
    "        ix2 = ix3\n",
    "    print(\"\".join(name_chars))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:15.017319Z",
     "start_time": "2024-03-29T11:21:15.002689Z"
    }
   },
   "id": "9db2ea5d4687c31",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.0931\n"
     ]
    }
   ],
   "source": [
    "loss = .0\n",
    "n = 0\n",
    "for word in words:\n",
    "    processed_word = [special_token] + list(word) + [special_token]\n",
    "    zipped_word = zip(*[processed_word[i:] for i in range(num_grams)])\n",
    "    for chars in zipped_word:\n",
    "        loss += torch.log(P[tuple(map(lambda x: stoi[x], chars))]).item()\n",
    "        n += 1\n",
    "print(f\"Loss: {-loss / n:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:16.360335Z",
     "start_time": "2024-03-29T11:21:15.019319Z"
    }
   },
   "id": "6593f58e8155994e",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient descent approach"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93a3c14d720fa657"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for word in words:\n",
    "    processed_word = [special_token] + list(word) + [special_token]\n",
    "    zipped_word = zip(*[processed_word[i:] for i in range(num_grams)])\n",
    "    for chars in zipped_word:\n",
    "        xs.append(list(map(lambda x: stoi[x], chars[:-1])))\n",
    "        ys.append(stoi[chars[-1]])\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num_examples = xs.nelement()\n",
    "xenc = F.one_hot(xs, num_classes=len(stoi)).float().flatten(1, 2)\n",
    "\n",
    "g = torch.Generator().manual_seed(0)\n",
    "W = torch.randn((len(stoi) * (num_grams - 1), len(stoi)), generator=g, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:16.938080Z",
     "start_time": "2024-03-29T11:21:16.361330Z"
    }
   },
   "id": "fbee03df51d3ca99",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.3783\n",
      "Loss: 3.5119\n",
      "Loss: 3.1645\n",
      "Loss: 2.9630\n",
      "Loss: 2.8306\n",
      "Loss: 2.7412\n",
      "Loss: 2.6765\n",
      "Loss: 2.6273\n",
      "Loss: 2.5881\n",
      "Loss: 2.5562\n",
      "Loss: 2.5296\n",
      "Loss: 2.5070\n",
      "Loss: 2.4876\n",
      "Loss: 2.4708\n",
      "Loss: 2.4560\n",
      "Loss: 2.4430\n",
      "Loss: 2.4314\n",
      "Loss: 2.4210\n",
      "Loss: 2.4117\n",
      "Loss: 2.4032\n",
      "Loss: 2.3955\n",
      "Loss: 2.3885\n",
      "Loss: 2.3821\n",
      "Loss: 2.3762\n",
      "Loss: 2.3707\n",
      "Loss: 2.3656\n",
      "Loss: 2.3609\n",
      "Loss: 2.3565\n",
      "Loss: 2.3525\n",
      "Loss: 2.3487\n",
      "Loss: 2.3451\n",
      "Loss: 2.3417\n",
      "Loss: 2.3386\n",
      "Loss: 2.3356\n",
      "Loss: 2.3328\n",
      "Loss: 2.3302\n",
      "Loss: 2.3277\n",
      "Loss: 2.3253\n",
      "Loss: 2.3231\n",
      "Loss: 2.3209\n",
      "Loss: 2.3189\n",
      "Loss: 2.3170\n",
      "Loss: 2.3152\n",
      "Loss: 2.3134\n",
      "Loss: 2.3117\n",
      "Loss: 2.3101\n",
      "Loss: 2.3086\n",
      "Loss: 2.3072\n",
      "Loss: 2.3058\n",
      "Loss: 2.3044\n",
      "Loss: 2.3032\n",
      "Loss: 2.3019\n",
      "Loss: 2.3007\n",
      "Loss: 2.2996\n",
      "Loss: 2.2985\n",
      "Loss: 2.2975\n",
      "Loss: 2.2964\n",
      "Loss: 2.2955\n",
      "Loss: 2.2945\n",
      "Loss: 2.2936\n",
      "Loss: 2.2927\n",
      "Loss: 2.2919\n",
      "Loss: 2.2911\n",
      "Loss: 2.2903\n",
      "Loss: 2.2895\n",
      "Loss: 2.2888\n",
      "Loss: 2.2881\n",
      "Loss: 2.2874\n",
      "Loss: 2.2867\n",
      "Loss: 2.2861\n",
      "Loss: 2.2854\n",
      "Loss: 2.2848\n",
      "Loss: 2.2842\n",
      "Loss: 2.2837\n",
      "Loss: 2.2831\n",
      "Loss: 2.2825\n",
      "Loss: 2.2820\n",
      "Loss: 2.2815\n",
      "Loss: 2.2810\n",
      "Loss: 2.2805\n",
      "Loss: 2.2800\n",
      "Loss: 2.2796\n",
      "Loss: 2.2791\n",
      "Loss: 2.2787\n",
      "Loss: 2.2783\n",
      "Loss: 2.2779\n",
      "Loss: 2.2775\n",
      "Loss: 2.2771\n",
      "Loss: 2.2767\n",
      "Loss: 2.2763\n",
      "Loss: 2.2760\n",
      "Loss: 2.2756\n",
      "Loss: 2.2752\n",
      "Loss: 2.2749\n",
      "Loss: 2.2746\n",
      "Loss: 2.2743\n",
      "Loss: 2.2739\n",
      "Loss: 2.2736\n",
      "Loss: 2.2733\n",
      "Loss: 2.2730\n",
      "Loss: 2.2727\n",
      "Loss: 2.2725\n",
      "Loss: 2.2722\n",
      "Loss: 2.2719\n",
      "Loss: 2.2716\n",
      "Loss: 2.2714\n",
      "Loss: 2.2711\n",
      "Loss: 2.2709\n",
      "Loss: 2.2706\n",
      "Loss: 2.2704\n",
      "Loss: 2.2702\n",
      "Loss: 2.2699\n",
      "Loss: 2.2697\n",
      "Loss: 2.2695\n",
      "Loss: 2.2693\n",
      "Loss: 2.2691\n",
      "Loss: 2.2689\n",
      "Loss: 2.2686\n",
      "Loss: 2.2684\n",
      "Loss: 2.2683\n",
      "Loss: 2.2681\n",
      "Loss: 2.2679\n",
      "Loss: 2.2677\n",
      "Loss: 2.2675\n",
      "Loss: 2.2673\n",
      "Loss: 2.2671\n",
      "Loss: 2.2670\n",
      "Loss: 2.2668\n",
      "Loss: 2.2666\n",
      "Loss: 2.2665\n",
      "Loss: 2.2663\n",
      "Loss: 2.2662\n",
      "Loss: 2.2660\n",
      "Loss: 2.2658\n",
      "Loss: 2.2657\n",
      "Loss: 2.2655\n",
      "Loss: 2.2654\n",
      "Loss: 2.2653\n",
      "Loss: 2.2651\n",
      "Loss: 2.2650\n",
      "Loss: 2.2648\n",
      "Loss: 2.2647\n",
      "Loss: 2.2646\n",
      "Loss: 2.2644\n",
      "Loss: 2.2643\n",
      "Loss: 2.2642\n",
      "Loss: 2.2641\n",
      "Loss: 2.2639\n",
      "Loss: 2.2638\n",
      "Loss: 2.2637\n",
      "Loss: 2.2636\n",
      "Loss: 2.2635\n",
      "Loss: 2.2634\n",
      "Loss: 2.2632\n",
      "Loss: 2.2631\n",
      "Loss: 2.2630\n",
      "Loss: 2.2629\n",
      "Loss: 2.2628\n",
      "Loss: 2.2627\n",
      "Loss: 2.2626\n",
      "Loss: 2.2625\n",
      "Loss: 2.2624\n",
      "Loss: 2.2623\n",
      "Loss: 2.2622\n",
      "Loss: 2.2621\n",
      "Loss: 2.2620\n",
      "Loss: 2.2619\n",
      "Loss: 2.2618\n",
      "Loss: 2.2617\n",
      "Loss: 2.2617\n",
      "Loss: 2.2616\n",
      "Loss: 2.2615\n",
      "Loss: 2.2614\n",
      "Loss: 2.2613\n",
      "Loss: 2.2612\n",
      "Loss: 2.2611\n",
      "Loss: 2.2611\n",
      "Loss: 2.2610\n",
      "Loss: 2.2609\n",
      "Loss: 2.2608\n",
      "Loss: 2.2608\n",
      "Loss: 2.2607\n",
      "Loss: 2.2606\n",
      "Loss: 2.2605\n",
      "Loss: 2.2605\n",
      "Loss: 2.2604\n",
      "Loss: 2.2603\n",
      "Loss: 2.2602\n",
      "Loss: 2.2602\n",
      "Loss: 2.2601\n",
      "Loss: 2.2600\n",
      "Loss: 2.2600\n",
      "Loss: 2.2599\n",
      "Loss: 2.2598\n",
      "Loss: 2.2598\n",
      "Loss: 2.2597\n",
      "Loss: 2.2596\n",
      "Loss: 2.2596\n",
      "Loss: 2.2595\n",
      "Loss: 2.2595\n",
      "Loss: 2.2594\n",
      "Loss: 2.2593\n",
      "Loss: 2.2593\n",
      "Loss: 2.2592\n",
      "Loss: 2.2592\n",
      "Loss: 2.2591\n",
      "Loss: 2.2591\n",
      "Loss: 2.2590\n",
      "Loss: 2.2589\n",
      "Loss: 2.2589\n",
      "Loss: 2.2588\n",
      "Loss: 2.2588\n",
      "Loss: 2.2587\n",
      "Loss: 2.2587\n",
      "Loss: 2.2586\n",
      "Loss: 2.2586\n",
      "Loss: 2.2585\n",
      "Loss: 2.2585\n",
      "Loss: 2.2584\n",
      "Loss: 2.2584\n",
      "Loss: 2.2583\n",
      "Loss: 2.2583\n",
      "Loss: 2.2582\n",
      "Loss: 2.2582\n",
      "Loss: 2.2581\n",
      "Loss: 2.2581\n",
      "Loss: 2.2581\n",
      "Loss: 2.2580\n",
      "Loss: 2.2580\n",
      "Loss: 2.2579\n",
      "Loss: 2.2579\n",
      "Loss: 2.2578\n",
      "Loss: 2.2578\n",
      "Loss: 2.2578\n",
      "Loss: 2.2577\n",
      "Loss: 2.2577\n",
      "Loss: 2.2576\n",
      "Loss: 2.2576\n",
      "Loss: 2.2575\n",
      "Loss: 2.2575\n",
      "Loss: 2.2575\n",
      "Loss: 2.2574\n",
      "Loss: 2.2574\n",
      "Loss: 2.2574\n",
      "Loss: 2.2573\n",
      "Loss: 2.2573\n",
      "Loss: 2.2572\n",
      "Loss: 2.2572\n",
      "Loss: 2.2572\n",
      "Loss: 2.2571\n",
      "Loss: 2.2571\n",
      "Loss: 2.2571\n",
      "Loss: 2.2570\n",
      "Loss: 2.2570\n",
      "Loss: 2.2570\n",
      "Loss: 2.2569\n",
      "Loss: 2.2569\n",
      "Loss: 2.2569\n",
      "Loss: 2.2568\n",
      "Loss: 2.2568\n",
      "Loss: 2.2568\n",
      "Loss: 2.2567\n",
      "Loss: 2.2567\n",
      "Loss: 2.2567\n",
      "Loss: 2.2567\n",
      "Loss: 2.2566\n",
      "Loss: 2.2566\n",
      "Loss: 2.2566\n",
      "Loss: 2.2565\n",
      "Loss: 2.2565\n",
      "Loss: 2.2565\n",
      "Loss: 2.2564\n",
      "Loss: 2.2564\n",
      "Loss: 2.2564\n",
      "Loss: 2.2564\n",
      "Loss: 2.2563\n",
      "Loss: 2.2563\n",
      "Loss: 2.2563\n",
      "Loss: 2.2563\n",
      "Loss: 2.2562\n",
      "Loss: 2.2562\n",
      "Loss: 2.2562\n",
      "Loss: 2.2562\n",
      "Loss: 2.2561\n",
      "Loss: 2.2561\n",
      "Loss: 2.2561\n",
      "Loss: 2.2561\n",
      "Loss: 2.2560\n",
      "Loss: 2.2560\n",
      "Loss: 2.2560\n",
      "Loss: 2.2560\n",
      "Loss: 2.2559\n",
      "Loss: 2.2559\n",
      "Loss: 2.2559\n",
      "Loss: 2.2559\n",
      "Loss: 2.2558\n",
      "Loss: 2.2558\n",
      "Loss: 2.2558\n",
      "Loss: 2.2558\n",
      "Loss: 2.2557\n",
      "Loss: 2.2557\n",
      "Loss: 2.2557\n",
      "Loss: 2.2557\n",
      "Loss: 2.2557\n",
      "Loss: 2.2556\n",
      "Loss: 2.2556\n",
      "Loss: 2.2556\n",
      "Loss: 2.2556\n",
      "Loss: 2.2556\n",
      "Loss: 2.2555\n",
      "Loss: 2.2555\n",
      "Loss: 2.2555\n",
      "Loss: 2.2555\n",
      "Loss: 2.2555\n",
      "Loss: 2.2554\n",
      "Loss: 2.2554\n",
      "Loss: 2.2554\n",
      "Loss: 2.2554\n",
      "Loss: 2.2554\n",
      "Loss: 2.2553\n",
      "Loss: 2.2553\n",
      "Loss: 2.2553\n",
      "Loss: 2.2553\n",
      "Loss: 2.2553\n",
      "Loss: 2.2553\n",
      "Loss: 2.2552\n",
      "Loss: 2.2552\n",
      "Loss: 2.2552\n",
      "Loss: 2.2552\n",
      "Loss: 2.2552\n",
      "Loss: 2.2552\n",
      "Loss: 2.2551\n",
      "Loss: 2.2551\n",
      "Loss: 2.2551\n",
      "Loss: 2.2551\n",
      "Loss: 2.2551\n",
      "Loss: 2.2551\n",
      "Loss: 2.2550\n",
      "Loss: 2.2550\n",
      "Loss: 2.2550\n",
      "Loss: 2.2550\n",
      "Loss: 2.2550\n",
      "Loss: 2.2550\n",
      "Loss: 2.2549\n",
      "Loss: 2.2549\n",
      "Loss: 2.2549\n",
      "Loss: 2.2549\n",
      "Loss: 2.2549\n",
      "Loss: 2.2549\n",
      "Loss: 2.2549\n",
      "Loss: 2.2548\n",
      "Loss: 2.2548\n",
      "Loss: 2.2548\n",
      "Loss: 2.2548\n",
      "Loss: 2.2548\n",
      "Loss: 2.2548\n",
      "Loss: 2.2548\n",
      "Loss: 2.2547\n",
      "Loss: 2.2547\n",
      "Loss: 2.2547\n",
      "Loss: 2.2547\n",
      "Loss: 2.2547\n",
      "Loss: 2.2547\n",
      "Loss: 2.2547\n",
      "Loss: 2.2547\n",
      "Loss: 2.2546\n",
      "Loss: 2.2546\n",
      "Loss: 2.2546\n",
      "Loss: 2.2546\n",
      "Loss: 2.2546\n",
      "Loss: 2.2546\n",
      "Loss: 2.2546\n",
      "Loss: 2.2546\n",
      "Loss: 2.2545\n",
      "Loss: 2.2545\n",
      "Loss: 2.2545\n",
      "Loss: 2.2545\n",
      "Loss: 2.2545\n",
      "Loss: 2.2545\n",
      "Loss: 2.2545\n",
      "Loss: 2.2545\n",
      "Loss: 2.2545\n",
      "Loss: 2.2544\n",
      "Loss: 2.2544\n",
      "Loss: 2.2544\n",
      "Loss: 2.2544\n",
      "Loss: 2.2544\n",
      "Loss: 2.2544\n",
      "Loss: 2.2544\n",
      "Loss: 2.2544\n",
      "Loss: 2.2544\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2543\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2542\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2541\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2540\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2539\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2538\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2537\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2536\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2535\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2534\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2533\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2532\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2531\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2530\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2529\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2528\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2527\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2526\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n",
      "Loss: 2.2525\n"
     ]
    }
   ],
   "source": [
    "regularization_param = 0.01\n",
    "alpha = 50\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(probs)), ys].log().mean() + regularization_param * (W ** 2).mean()\n",
    "    print(f\"Loss: {loss:.4f}\")\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -alpha * W.grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:42.833082Z",
     "start_time": "2024-03-29T11:21:16.941066Z"
    }
   },
   "id": "faedfe87b08265db",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "bhria\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "evackey\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "ren\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "ialle\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "uovie\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "yonna\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "ia\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "la\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "mallyshonialaytty\n",
      "tensor([[-4.0416,  2.5324, -0.2432, -0.2364,  0.0305,  2.4533, -1.2079, -1.3379,\n",
      "          1.0737,  1.3821, -0.7340, -1.0008,  0.4592,  1.5101, -0.8770,  0.7643,\n",
      "         -1.2371, -2.4235,  1.2894,  0.2276, -0.0208,  2.1168,  0.4154, -1.4197,\n",
      "         -0.7245,  1.3680,  1.0385]], grad_fn=<MmBackward0>)\n",
      "zade\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "g = torch.Generator().manual_seed(0)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([[ix1, ix2]]), num_classes=len(stoi)).float().flatten(1, 2)\n",
    "        logits = xenc @ W\n",
    "        # softmax\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "        ix3 = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        out.append(itos[ix3])\n",
    "        ix1 = ix2\n",
    "        ix2 = ix3\n",
    "    print(''.join(out))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:42.864645Z",
     "start_time": "2024-03-29T11:21:42.834081Z"
    }
   },
   "id": "58c5b8379ae2382",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2.252534866333008"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:42.880466Z",
     "start_time": "2024-03-29T11:21:42.865596Z"
    }
   },
   "id": "945dc347a034476f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T11:21:42.896362Z",
     "start_time": "2024-03-29T11:21:42.881466Z"
    }
   },
   "id": "9bc65559581961b2",
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
